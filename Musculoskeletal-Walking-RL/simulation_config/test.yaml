#DONT DELET THIS LINE
#The first line is reserved for internal usage. Don't write anything important there and leave it unchanged.
#file name of all configuration yaml file must only contain LOWERCASE

#Parallel Simulation(multiple backends):
#Prepare different configuration files in the following way.
#You can only specify the values which are changed compared with the default configuration.   

DDPG_Agent:
  #For ActorNet and CriticNet, any structure of fully connected layers is supported.
  #hidden_layers is written as yaml-style array, you can add more layers by expanding the first dim. of the array.
  #the seconde dim. of the array specifies the number of neurons in this layer and the corresponding activation function.   
  ActorNet:
    hidden_layers:
      -
        - 128
        - relu
      -
        - 128
        - relu
      #-
      #  - 128
      #  - relu
    output_activation: sigmoid

  CriticNet:
    hidden_layers:
      #-
      #  - 512
      #  - relu
      -
        - 256
        - relu
      -
        - 128
        - relu 
    output_activation: linear

  random_process: OrnsteinUhlenbeckProcess(theta=.15, mu=0., sigma=0.5, size=nA) #size=nA should not be changed

  memory: SequentialMemory(limit=100000, window_length=1)

  agent:
    #keyword arguments for rl.agents.DDPGAgent()
    nb_steps_warmup_critic: 100
    nb_steps_warmup_actor: 100
    gamma: .99
    batch_size: 5
    target_model_update: 0.001
    delta_clip: 1.

  optimizer:
    Adam(lr=.001, clipnorm=1.)
  compiler:
    #keyword arguments for compile()
    metrics: 
      - mae

  weights_sav_path:
  #don't use relative path, only contain lower case!!!
    #~/.opt/nrpStorage/Neurorobotics-Musculoskeletal-Walking-Robot-DRL_0
    ~/.opt/weights

Environment:
  reward_function:
    np.clip(3*link_vlin[0].x-abs(link_vlin[0].y)-1*abs(link_vlin[0].z)-sum(action)/24,-50,50)

Training:   
  Max_Epoch: 1000
  Max_Iteration_per_Epoch: 200